# -*- coding: utf-8 -*-
"""Fetch reddit data.ipynb

Automatically generated by Colaboratory. *modified*

Original file is located at
    https://colab.research.google.com/drive/1P8c6m1VOPcwpBUUGuRjBKpkf3-FmYzM3
"""

import logging

logging.basicConfig(filename='fetch_reddit_data.py.log',
                    filemode='a',
                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
                    datefmt='%H:%M:%S',
                    level=logging.INFO)

import requests
from datetime import datetime, timezone, timedelta
# import matplotlib.pyplot as plt
import pandas as pd
# !pip install psaw
from psaw import PushshiftAPI
import os

def getUTCDay(targetdate):
  return int(targetdate.timestamp() / (3600*24))

def diffMonths(date1, date2):
  return ((date1.year - date2.year) * 12) + date1.month - date2.month


def last_day_of_month(any_day):
    # get close to the end of the month for any day, and add 4 days 'over'
    next_month = any_day.replace(day=28) + timedelta(days=4)
    # subtract the number of remaining 'overage' days to get last day of current month, or said programattically said, the previous day of the first of next month
    return next_month - timedelta(days=next_month.day)

def getSubscriberCountPerDay(subredditname, firstDay, lastDay):
  response = requests.get('https://subredditstats.com/api/subreddit?name=' + subredditname)
  responsejson = response.json()
  filteredCounts = (day for day in responsejson.get('subscriberCountTimeSeries') if getUTCDay(firstDay) <= day.get('utcDay') <= getUTCDay(lastDay))
  return pd.DataFrame(filteredCounts)

def getSubscriberCountPerMonth(subredditname, firstDay, lastDay):
  perday = getSubscriberCountPerDay(subredditname, firstDay, lastDay)

  perday['month'] = perday['utcDay'].map(lambda day: diffMonths(datetime.fromtimestamp(day * 3600 * 24), firstDay))
  permonth = perday.groupby('month').sum()

  permonth['firstday'] = perday.groupby('month').min()['utcDay'].map(lambda day: datetime.fromtimestamp(day * 3600 * 24))
  return permonth.drop(columns='utcDay')

api = PushshiftAPI()

def getKeywordUsage(keyword, start_epoch, end_epoch):
  # return requests.get(str.format('https://api.pushshift.io/reddit/search/comment/?q={}&after={}&before={}&metadata=true&size=0', keyword, start_epoch, end_epoch)).json().get('metadata').get('total_results')
  response = api.search_comments(q=keyword, limit=0, after=start_epoch, before=end_epoch) # asking for 0 results; only doing this for the metadata
  list(response) # HACK to get the generator to activate(?) so that the library gets the metadata from the api
  countInComments = api.metadata_.get('total_results')

  response = api.search_submissions(q=keyword, limit=0, after=start_epoch, before=end_epoch)
  list(response)
  countInSubmissions = api.metadata_.get('total_results')
  return (countInComments, countInSubmissions)

def getUsageForKeyword(word):
  usageCounts = []
  currFirstDay = startDate
  for currLastDay in pd.date_range(startDate, endDate, freq='M', closed='left', tz=timezone.utc):
      start_epoch = int(currFirstDay.timestamp())
      end_epoch = int(currLastDay.timestamp())

      usageInComments, usageInPosts = getKeywordUsage(word, start_epoch, end_epoch)
      newEntry = { 'firstday': currFirstDay, 'lastday': currLastDay, 'countincomments': usageInComments, 'countinposts': usageInPosts }
      logging.info(newEntry)
      usageCounts.append(newEntry)

      currFirstDay = currLastDay
  return pd.DataFrame(usageCounts)

startDate = datetime(2012, 1, 1, tzinfo=timezone.utc)
endDate = datetime(2021, 3, 31, tzinfo=timezone.utc)

def downloadSubsCounts(name):
  destpath = os.path.join(os.path.dirname(__file__), 'subreddits', name + '.csv')

  df = getSubscriberCountPerMonth(name, startDate, endDate)
  # df.plot(x='firstday')
  df.to_csv(destpath)

def downloadKeywordCounts(name):
  destpath = os.path.join(os.path.dirname(__file__), 'keywords', name + '.csv')

  df2 = getUsageForKeyword(name)
  # # df2.plot(x='firstday', y='countincomments')
  # # df2.plot(x='firstday', y='countinposts')
  df2.to_csv(destpath)

# manually added quotation marks at comma characters
# limited to up to the first 9 of each category since the session seems to be timing out 
#keywords = ["negative", "kms", "kys", "stressed", "stress", "anxious", "anxiety", "fear", "scared", # "sad", "worried", "depressed", "depression", "sadness", "grief", "insane", "mad", "death", "murder", "kill", "suicide", "suicidal", "self-destruction", "felo-de-se", "unalive", "done with this", "had enough", "fed up", "tired", "ill", "numb", "tired", "fatigued", "angry", "allergic", "sick", "consternated", "upset", "confused", "confusion", "bad", "OCD", "insomnia", "addiction", "narcolepsy", "alone", "lonely", "loneliness", "isolated", "isolation", "heartbroken", "hopeless", "furious", "ashamed", "sorrow", "cry", "crying", "self-destruction", "self-destroy", "afraid", "drugs", "gloomy", "melancholy", "dejected", "hate", "insecure", "unproductive", "burnt out", "joyless", "aimless", "empty", "stagnant", "terrible", "awful", "poor", "bullies", "bullying", "threatening", "threaten", "cheating", "cheat", "sabotage", "rape", "sexual assault", "assault", "mad", "hurt", "hurting", "cancer", "disease", "famine", "war", "hurting", "nightmare", "worst", "fall apart", "hardship", "difficulty", "lazy", "unprotected", "break down", "breakdown", "unloved", "violated", "vulnerable", "unaccepted", "withdrawn", "betray", "betrayal", "betrayed", "trash", "resentful", "resentment", "embarrassed", "embarrassment", "screwed", "ruined", "ruin", "helpless", "guilty", "frightened", "humiliated", "fail", "failure", "fearful", "misunderstood", "mistake", "criticized", "deceived", "attacked", "bitter", "agony", "accused", "accident", "abandoned", "abandonment", "powerless", "panic", "panicky", "insult", "insecure", "incompetent", "suffocated", "smother", "put-down", "manipulated", "nasty", "neglected", "insignificant", "ridicule", "ridiculed", "forgotten", "foolish", "nervous", "wreck", "broken", "pathetic", ":(",
#"mental health", "pull through", "survive", "matter", "believe",
#"positive", "thriving", "wellness", "good", "great", "happy", "grateful", "amazing", "awesome"] # , "best", "hearty", "gratefulness", "fit", "enjoy", "enjoying", "excited", "excitement", "at peace", "glad", "content", "fortunate", "blissful", "bliss", "love", "lovely", "loving", "merry", "elated", "elation", "cheer", "cheery", "hope", "hopeful", "glee", "happy", "happiness", "loving", "healed", "bless", "killing it", "successful", "mirth", "joy", "joyous", "victory", "well-being", "festive", "festivities", "celebrate", "celebration", "laughing", "laugh", "charming", "charm", "jubilant", "yay", "woohoo", "like", "adore", "healthy", "confident", "confidence", "sexy", "LOL", "lmao", "lls", "haha", "hahaha", "lmfao", "awesome", "in love", "secure", "motivated", "peace", "peaceful", "yoga", "exercise", "optimistic", "productive", "productivity", "liberty", "free", "pride", "proud", "lucky", "luck", "calm", "community", "prosperous", "rich", "friends", "friend", "parties", "party", "lively", "spirited", "active", "exploring", "explore", "play", "playful", "rosy", "satisfaction", "satisfying", "exhilarated", "nice", "positive", "genial", "treasure", "dream", "fortunate", "happiness", "wealth", " meaning", "gratification", "ecstatic", "best", "upbeat", "uplift", "uplifted", "strong", "clever", "smart", "crafty", "witty", "insightful", "appreciated", "appreciation", "appreciate", "comfortable", "energetic", "energy", "powerful", "delight", "delightful", "delighted", "impressed", "cute", "pretty", "hallelujah", "amuse", "amused", "inspirational", "inspired", "inspire", "cozy", "togetherness", "upbeat", "camaraderie", "fresh", "lighthearted", "magical", "grace", "gracious", "delicious", "yummy", "soothing", "smile", "smiling", "quality", "sunset", "sunrise", "overjoyed", "sunshine", "thank you", "thanks", "thankful", "welcome", "diverse", "included", "diversity", "inclusion", ":)"]
keywords = ["*"] # to get all words

# ran JS:
# notionpage = `...` // <- the markdown of the chunk of the lexicon/subreddits list page on notion containing all the subreddits
# console.log(JSON.stringify(Array.from(new Set(notionpage.match(/(?:\/r\/)([\w\d]+)/g).map(str => str.replace(/^\/r\//, ''))))))
# manually removed items crossed out on the notion page
subreddits = ["Astronomy","fishtank","Aquariums","Jarrariums","nanotank","fishing","flyfishing","Archery","woodworking","HomeImprovement","watchmaking","blacksmith","metalworking","art","painting","drawing","happytrees","pixelart","Zentangle","homebrewing","cider","mead","firewater","TheBrewery","DIY","craft","DIY_tech","Buildapc","coding","learnprogramming","pcmasterrace","books","suggestmeabook","bookclub","52book","gaming","ps3","ps4","pcgaming","xbox360","xboxone","games","hunting","waterfowl","outdoors","camping","hiking","geocaching","penpals","MakeNewFriendsHere","boardgames","hearthstone","magicTCG","DnD","chess","writing","Screenwriting","Poetry","FreeWrite","CreativeWriting","lockpicking","truefilm","movies","Buddhism","Stoicism","atheism","trueatheism","Christianity","exmormon","Catholicism","lgbt","gaybros","actuallesbians","gaymers","bisexual","askgaybros","ainbow","gay","gay_irl","trans","MtF","ftm","nonbinary","TwoXChromosomes","askwomen","everymanshouldknow","askmen","feminism","asianamerican","AfricanAmerican","ABCDesis","Hispanic","GetMotivated","health","ZenHabits","motivation","fitmeals","paleo","nutrition","vegetarian","vegan","Cheap_Meals","HealthyFood","veganrecipes","baking","castiron","instantpot","mealprepsunday","breadit","cooking","slowcooking","bicycling","yoga","skateboarding","climbing","backpacking","bjj","skiing","crossfit","bodybuilding","WeightRoom","powerlifting","loseit","bodyweightfitness","gainit","swoleacceptance","flexibility","taekwondo","sports","running","runninglifestyle","golf","sportsarefun","tennis","rugbyunion","discgolf","cricket","sailing","Marvel","GameOfThrones","BreakingBad","thewalkingdead","community","arresteddevelopment","topgear","StarTrek","Treknobabble","HIMYM","firefly","PandR","Sherlock","DunderMifflin","BetterCallSaul","TrueDetective","houseofcards","MakingaMurderer","FlashTV","trailerparkboys","mrrobot","siliconvalleyhbo","strangerthings","supernatural","thegrandtour","AmericanHorrorStory","rupaulsdragrace","westworld","blackmirror","FilthyFrank","twinpeaks","bigbrother","brooklynninenine","scrubs","howyoudoin","30rock","lifeisstrange","survivor","riverdale","letterkenny","Pokemon","AdventureTime","futurama","TheLastAirbender","ArcherFX","southpark","TheSimpsons","mylittlepony","rickandmorty","naruto","stevenuniverse","onepunchman","BobsBurgers","BoJackHorseman","gravityfalls","familyguy","kingofthehill","spongebob","TaylorSwift","classicalmusic","jazz","indieheads","gamemusic","Metal","dubstep","electronicmusic","edmproduction","EDM","Drawing","crafts","calligraphy","handwriting","cars","motorcycles","survival","homestead","MTB","makeupaddiction","SkincareAddiction","malehairadvice","curlyhair","beards","wicked_edge","RedditLaqueristas","AsianBeauty","piercing","tattoos","tattoo","malefashionadvice","frugalmalefashion","femalefashionadvice","thriftstorehauls","succulents","mycology","bonsai","houseplants","earthporn","hardcoreaww","aww","cats","dogs","justneckbeardthings","neckbeardnests","corgi","Pitbulls","goldenretrievers"]

# for subreddit in subreddits:
#   logging.info("Downloading for subreddit: " + subreddit)
#   downloadSubsCounts(subreddit)

for keyword in keywords:
  logging.info("Downloading for keyword: " + keyword)
  downloadKeywordCounts(keyword)